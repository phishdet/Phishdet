{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"python_pandas.jpg\" alt=\"Python\" style=\"width: 400px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results of Applied ML techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nbi:hide_in\n",
    "import pandas as pd\n",
    "from ipywidgets import interact\n",
    "data=pd.read_excel(\"phisingdataset.xlsx\",header=None)\n",
    "data.columns=[\"having_IP_Address\",\"URL_Length\",\"Shortining_Service\",\"having_At_Symbol\",\"double_slash_redirecting\",\"Prefix_Suffix\",\n",
    "\"having_Sub_Domain\",\"SSLfinal_State\",\"Domain_registeration_length\",\"Favicon\",\"port\",\"HTTPS_token\",\"Request_URL\",\"URL_of_Anchor\",\n",
    "\"Links_in_tags\",\"SFH\",\"Submitting_to_email\",\"Abnormal_URL\",\"Redirect\",\"on_mouseover\",\"RightClick\",\"popUpWidnow\",\"Iframe\",\n",
    "\"age_of_domain\",\"DNSRecord\",\"web_traffic\",\"Page_Rank\",\"Google_Index\",\"Links_pointing_to_page\",\"Statistical_report\",\"Result\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nbi:hide_in\n",
    "# c) divide data in input and target\n",
    "Y=data[\"Result\"]\n",
    "X=data[[\"having_IP_Address\",\"URL_Length\",\"Shortining_Service\",\"having_At_Symbol\",\"double_slash_redirecting\",\"Prefix_Suffix\",\n",
    "\"having_Sub_Domain\",\"SSLfinal_State\",\"Domain_registeration_length\",\"Favicon\",\"port\",\"HTTPS_token\",\"Request_URL\",\"URL_of_Anchor\",\n",
    "\"Links_in_tags\",\"SFH\",\"Submitting_to_email\",\"Abnormal_URL\",\"Redirect\",\"on_mouseover\",\"RightClick\",\"popUpWidnow\",\"Iframe\",\n",
    "\"age_of_domain\",\"DNSRecord\",\"web_traffic\",\"Page_Rank\",\"Google_Index\",\"Links_pointing_to_page\",\"Statistical_report\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nbi:hide_in\n",
    "#  Normalisation of Data\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc=StandardScaler()\n",
    "standard_scaler=sc.fit_transform(X)\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "ms=MinMaxScaler()\n",
    "Min_Max_Scaler=ms.fit_transform(X)\n",
    "\n",
    "\n",
    "# Since all the values are range in between -1 to 1 so we don't need normalsation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nbi:hide_in\n",
    "x=data.iloc[:,0:30]\n",
    "y=data.iloc[:,30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nbi:hide_in\n",
    "from sklearn.model_selection import train_test_split\n",
    "(x_train,x_test,y_train,y_test)=train_test_split(x,y,train_size=0.7,random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracies of SVM Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "- Missclassified sample:  259\n",
    "Traning accuracy: 0.9302145257172396\n",
    "Testing accuracy: 0.9219173952366596\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "          -1       0.93      0.89      0.91      1498\n",
    "           1       0.91      0.95      0.93      1819\n",
    "\n",
    "    accuracy                           0.92      3317\n",
    "   macro avg       0.92      0.92      0.92      3317\n",
    "weighted avg       0.92      0.92      0.92      3317\n",
    "\n",
    "[[1338  160]\n",
    " [  99 1720]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "### Accuracies of Perceptron, Pipeline perceptron, Bagging, Adaboost and Ensemble"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "- Perceptron  :=  0.8504672897196262\n",
    "- Pipeline perceptron  :=  0.8857401266204401\n",
    "- Bagging  :=  0.9692493216762135\n",
    "- AdaBoost  :=  0.967138981006934\n",
    "- Ensemble  :=  0.9369912571600845"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nbi:hide_in\n",
    "def f(x):\n",
    "    \n",
    "    if(x=='Logistic Regression'):\n",
    "        # Logistic Regression\n",
    "\n",
    "        from sklearn.linear_model import LogisticRegression\n",
    "        from sklearn.metrics import classification_report, confusion_matrix\n",
    "        model = LogisticRegression(solver='liblinear', C=0.05,random_state=0)\n",
    "        model.fit(x_train, y_train)\n",
    "        y_pred = model.predict(x_test)\n",
    "        print(\"Predicted Output: \",y_pred)\n",
    "        training_score=model.score(x_train, y_train)\n",
    "        print(\"Training Score: \",training_score)\n",
    "        test_score=model.score(x_test, y_test)\n",
    "        print(\"Test Score: \",test_score)\n",
    "    elif(x=='SVM'):\n",
    "        # Applying SVM model\n",
    "        from sklearn.svm import SVC\n",
    "        sv=SVC(C=10,kernel='linear') # C is constant\n",
    "        sv.fit(x_train,y_train)\n",
    "        train_pred=sv.predict(x_train)\n",
    "        test_pred=sv.predict(x_test)\n",
    "        print(\"Missclassified sample: \",(test_pred!=y_test).sum())\n",
    "\n",
    "        # Computing accuracy\n",
    "        from sklearn.metrics import accuracy_score\n",
    "        train_acc=accuracy_score(y_train,train_pred)\n",
    "        test_acc=accuracy_score(y_test,test_pred)\n",
    "        print(\"Traning accuracy:\",train_acc)\n",
    "        print(\"Testing accuracy:\",test_acc)\n",
    "\n",
    "        # Classification report\n",
    "        from sklearn.metrics import classification_report\n",
    "        cr=classification_report(y_test,test_pred)\n",
    "        print(cr)\n",
    "\n",
    "        # confusion matrics\n",
    "        from sklearn.metrics import confusion_matrix\n",
    "        cm=confusion_matrix(y_test,test_pred)\n",
    "        print(cm)  \n",
    "    elif(x=='Random Forest'):\n",
    "        import numpy as np\n",
    "        from sklearn.ensemble import RandomForestClassifier\n",
    "        import matplotlib.pyplot as plt\n",
    "        feat_lables=data.columns[1:]\n",
    "        forest=RandomForestClassifier(n_estimators=10000,random_state=0)\n",
    "        forest.fit(x_train,y_train)\n",
    "        '''importances=forest.feature_importances_\n",
    "        indices=np.argsort(importances)[::-1]\n",
    "        for f in range (x_train.shape[1]):\n",
    "            print(\"(%2d) %-*s %f\"%(f+1,30,feat_lables[indices[f]],importances[indices[f]]))\n",
    "        plt.title(\"Feature Importances\")\n",
    "        plt.bar(range(x_train.shape[1]),\n",
    "                importances[indices],\n",
    "                color='lightblue',\n",
    "                align=\"center\")\n",
    "        plt.xticks(range(x_train.shape[1]),\n",
    "                   feat_lables[indices],rotation=90)\n",
    "        plt.xlim([-1,x_train.shape[1]])\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        '''\n",
    "        #################################\n",
    "        y_pred=forest.predict(x_test)\n",
    "        from sklearn.metrics import accuracy_score\n",
    "        acc=accuracy_score(y_test,y_pred)\n",
    "        print(\"Accuracy:\",acc)\n",
    "        from sklearn.metrics import confusion_matrix\n",
    "        cm=confusion_matrix(y_test,y_pred)\n",
    "        print(cm)\n",
    "        from sklearn.metrics import classification_report\n",
    "        cpr=classification_report(y_test,y_pred)\n",
    "        print(cpr)\n",
    "    else:\n",
    "        # nbi:hide_in\n",
    "        # implement Bagging classifier Perceptron , perceptron with pipelining \n",
    "        # Combine all these using majority voting classifier\n",
    "\n",
    "        from sklearn.linear_model import Perceptron\n",
    "        from sklearn.tree import DecisionTreeClassifier\n",
    "        from sklearn.pipeline import Pipeline\n",
    "        from sklearn.ensemble import BaggingClassifier\n",
    "        from sklearn.ensemble import AdaBoostClassifier\n",
    "        from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "        clf1=Perceptron()\n",
    "        clf2=Pipeline([['SC',StandardScaler()],['PPN',clf1]])\n",
    "        clf3=BaggingClassifier(base_estimator=DecisionTreeClassifier(),n_estimators=100)\n",
    "        clf4=AdaBoostClassifier(base_estimator=DecisionTreeClassifier(),n_estimators=100)\n",
    "        clf5=VotingClassifier(estimators=[('PPN',clf1),(\"Pipe\",clf2),(\"Bag\",clf3),(\"Ada\",clf4)]\n",
    "                              ,voting='hard'\n",
    "                              ,weights=[1,1,1,1])\n",
    "        clf=[clf1,clf2,clf3,clf4,clf5]\n",
    "        name=[\"Perceptron\",\"Pipeline perceptron\",\"Bagging\",\"AdaBoost\",\"Ensemble\"]\n",
    "        score=[]\n",
    "        from sklearn.metrics import accuracy_score\n",
    "        for  model in clf:\n",
    "            model.fit(x_train,y_train)\n",
    "            y_pred=model.predict(x_test)\n",
    "            acc=accuracy_score(y_test, y_pred)\n",
    "            score.append(acc)\n",
    "        for c,s in zip(name,score):\n",
    "            print(c,\" := \",s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56022de5468f4e70970494826919fb56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='x', options=('Perceptron', 'Pipeline perceptron', 'Logistic Regresâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.f(x)>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# nbi:hide_in\n",
    "interact(f,x=['SVM','Bagging','Adaboost','Ensemble','Random Forest','Perceptron','Pipeline perceptron','Logistic Regression'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
